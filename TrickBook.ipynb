{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. df['Rating_4-5'] = df['Item_Rating'].between(4,5,inclusive=True).astype(int) (For playing with ratings)(Binning them)\n",
    "\n",
    "2. # used to handle the skewness in the data\n",
    "   df['SalesInMillions'] = np.log1p(df['SalesInMillions']) (Handling skewness with log transformation)\n",
    "   sns.distplot(df['SalesInMillions']) (Plotting to see skewness)\n",
    "   y_pred1=np.expm1(y_pred1) (inverse tranforming the output results)\n",
    "    \n",
    "3. #code to draw a heatmap\n",
    "   corr = df.corr()\n",
    "   sns.heatmap(corr,annot=True)\n",
    "\n",
    "3. # In order to split a string a string in the columns and to get specific characters out from it\n",
    "   banned = ['out','of','5','stars']\n",
    "   f = lambda x:' '.join([item for item in x.split() if item not in banned])\n",
    "   df['Reviews'] = df['Reviews'].apply(f)\n",
    "   df['Reviews']\n",
    "\n",
    "4. If there are 5-10 or lesser big outliers in the data then remove them without any second thoughts as they will highly sway  \n",
    "   your results.\n",
    "\n",
    "5. # using sweeetviz to do initial data analysis\n",
    "   my_report = sweetviz.compare([df.drop(['YEAR','ID'],axis=1), \"Train\"], [test.drop(['YEAR','ID'],axis=1), \"Test\"],              \"SalesInMillions\")\n",
    "   my_report.show_html('Report.html')\n",
    "\n",
    "6. #website for regular expressions tutorial\n",
    "   https://www.coursera.org/learn/python-text-mining/lecture/sVe8B/regular-expressions\n",
    "   new_df['just_movie_titles'] =        df['movie_title'].str.extract('(.+?) \\(')\n",
    "\n",
    "\n",
    "7. #finding index of all the rows with some null values\n",
    "   inds = pd.isnull(df).any(1).nonzero()[0]\n",
    "\n",
    "8. #way to find all the Null Values\n",
    "   df.info()\n",
    "   df.isnull().sum(axis=0)\n",
    "\n",
    "9. #finding error keys\n",
    "   import sklearn\n",
    "   sorted(sklearn.metrics.SCORERS.keys()) \n",
    "\n",
    "10. #reading the dataframe back to csv file\n",
    "    pred_final2 = pd.DataFrame(gsearch5.best_estimator_.predict_proba(df_final))\n",
    "    pred_final2.to_excel('ODI_Participants_data/Sample_submission.xlsx')\n",
    "    pred_final2.head()\n",
    "\n",
    "11. #using SMOTE from imblearn for imbalanced classes\n",
    "    X = df.drop('Cover_Type',axis=1)\n",
    "    Y = df['Cover_Type']\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.25)\n",
    "    sm = SMOTE(random_state=0)\n",
    "    X_train,y_train = sm.fit_sample(X_train,y_train)\n",
    "\n",
    "12. #seed samppling for xgboost\n",
    "    preds = 0\n",
    "    for seed_val in range(100):\n",
    "        print (seed_val)\n",
    "        m= XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=17,\n",
    "                        min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=seed_val)\n",
    "        m.fit(X,Y)\n",
    "        predict=m.predict(test)\n",
    "        preds += predict\n",
    "    preds = pd.DataFrame(preds/100)\n",
    "\n",
    "13. #preprocessing outliers using wincerization\n",
    "    upperbound , lowerbound = np.percentile(x,[1,99])\n",
    "    y = np.clip(x,upperbound,lowerbound)\n",
    "    pd.Series(y).hist(bins=30) #simplt shows the new distribution\n",
    "\n",
    "14. #rank transformation can be used in place of min max scaling as min max scaling is affected by outliers\n",
    "15. #use frequency encoding for categorical data as it'll tell how many times a feature occured. Don't use it when features have same frequency\n",
    "16. #use x.value_counts(),x.isnull(),x.dtypes for looking into the dataset\n",
    "17. #label encoding and one hot encoding\n",
    "    o_e = OneHotEncoder(sparse=False)\n",
    "    l_e = LabelEncoder()\n",
    "    df['Team_Venue'] = l_e.fit_transform(df['Team1_Venue'])\n",
    "    oh_cols_train = pd.DataFrame(o_e.fit_transform(df[['Team1','Team_Venue','Team1_Innings','Team2']])) \n",
    "    oh_cols_train.drop(columns=[0,16],inplace=True)\n",
    "    df = pd.concat([oh_cols_train,df],axis=1)\n",
    "18. #playing with dates\n",
    "    rr = pd.read_csv('RR.csv')\n",
    "    rr['hour_min'] = rr['time'].str.extract('(\\d{2}:\\d{2})')\n",
    "    rr['time'] = pd.to_datetime(rr['time'], format='%H:%M:%S').dt.time\n",
    "    rr['hour'] = pd.to_datetime(rr['hour_min'], format='%H:%M').dt.hour\n",
    "    rr['min'] = pd.to_datetime(rr['hour_min'], format='%H:%M').dt.minute\n",
    "    #extracting year from date\n",
    "    df_m['listing_year'] = pd.to_datetime(df_m['listing_date']).dt.year\n",
    "    df_m['issue_year'] = pd.to_datetime(df_m['issue_date']).dt.year\n",
    "19. #selecting categorical and numeric columns from a dataset\n",
    "    cat_colc = list(train.select_dtypes(include=['object']).columns)\n",
    "    num_colc = list(train.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3cf04a790655>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-3cf04a790655>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    e-2, 0.1, 1, 100],'learning_rate':[i/10.0 for i in range(0,5)]}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#parameter tuning for xgboost\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='neg_root_mean_squared_error',n_jobs=-1, cv=5)\n",
    "gsearch1.fit(X,Y)\n",
    "gsearch1.best_params_,gsearch1.best_score_\n",
    "\n",
    "param_test2 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=gsearch1.best_params_['max_depth'],\n",
    " min_child_weight=gsearch1.best_params_['min_child_weight'], gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test2, scoring='f1_weighted',n_jobs=-1, cv=5)\n",
    "gsearch2.fit(X,Y1)\n",
    "gsearch2.best_params_,gsearch2.best_score_\n",
    "\n",
    "param_test3 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=gsearch1.best_params_['max_depth'],\n",
    " min_child_weight=gsearch1.best_params_['min_child_weight'], gamma=gsearch2.best_params_['gamma'], subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring='f1_weighted',n_jobs=-1, cv=5)\n",
    "gsearch3.fit(X,Y1)\n",
    "gsearch3.best_params_,gsearch3.best_score_\n",
    "\n",
    "param_test4 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],'learning_rate':[i/10.0 for i in range(0,5)]}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=gsearch1.best_params_['max_depth'],\n",
    " min_child_weight=gsearch1.best_params_['min_child_weight'], gamma=gsearch2.best_params_['gamma'], subsample=gsearch3.best_params_['subsample'], colsample_bytree=gsearch3.best_params_['colsample_bytree'], nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test4, scoring='f1_weighted',n_jobs=-1, cv=5)\n",
    "gsearch4.fit(X,Y1)\n",
    "gsearch4.best_params_,gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various libraries in python I have used till now\n",
    "# preprocessing and paramtere tuning libraries\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# model\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from skleanrn.linear_model import LinearRegression\n",
    "\n",
    "import sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5dc250f5e5b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#playing with files in python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtraining_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "#playing with files in python\n",
    "\n",
    "with open(filename) as training_file:\n",
    "        reader = csv.reader(training_file, delimiter=',')    \n",
    "        imgs = []\n",
    "        labels = []\n",
    "\n",
    "        next(reader, None)\n",
    "        \n",
    "        for row in reader:\n",
    "            label = row[0]\n",
    "            data = row[1:]\n",
    "            img = np.array(data).reshape((28, 28))\n",
    "\n",
    "            imgs.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "        images = np.array(imgs).astype(float)\n",
    "        labels = np.array(labels).astype(float)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed sampling\n",
    "preds = 0\n",
    "for seed_val in range(100):\n",
    "    print (seed_val)\n",
    "    m= XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=5, gamma=0.0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=seed_val)\n",
    "    m.fit(X,Y)\n",
    "    predict=m.predict(test)\n",
    "    preds+=np.expm1(predict)\n",
    "preds = pd.DataFrame(preds/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#importtant courses or places for revision\n",
    "1. (#decision tree explanation)https://courses.analyticsvidhya.com/courses/take/getting-started-with-decision-trees/lessons/8157460-terminologies-related-to-decision-trees\n",
    "2. how to win kaggle competitions(coursera)\n",
    "3. data science specialization from michigan university(coursera)\n",
    "4. Deep learning specialization by deeplearning.ai(coursera)\n",
    "5. applied tensorflow specialization by deeplearning.ai(coursera)\n",
    "6. https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/\n",
    "7. https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "8. https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "9. https://github.com/kunalj101/Data-Science-Hacks?utm_source=blog&utm_medium=git-github-essential-guide-beginners\n",
    "10. https://stats.stackexchange.com/questions/107610/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution#:~:text=Correspondingly%2C%20if%20you%20apply%20the%20log-transformation%20to%20something,So%20the%20log%20transformation%20wouldn%27t%20be%20helpful%20then.\n",
    "11. https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseliner(X, y, cv=3, metric='neg_log_loss'):\n",
    "    print(\"Baseliner Models\\n\")\n",
    "    eval_dict = {}\n",
    "    models = [lgb.LGBMClassifier(), xgb.XGBClassifier(), GradientBoostingClassifier(),\n",
    "                  LogisticRegression(), GaussianNB(), RandomForestClassifier(), DecisionTreeClassifier(),\n",
    "                  ExtraTreeClassifier(), AdaBoostClassifier(), BaggingClassifier(), ExtraTreesClassifier(),\n",
    "              SVC(probability=True), KNeighborsClassifier() \n",
    "                 ]\n",
    "    print(\"Model Name \\t |   CV\")\n",
    "    print(\"--\" * 50)\n",
    "\n",
    "    for index, model in enumerate(models, 0):\n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        eval_dict[model_name] = {}\n",
    "\n",
    "        results = cross_val_score(model, X, y, cv=cv, scoring=metric)\n",
    "        eval_dict[model_name]['cv'] = results.mean()\n",
    "\n",
    "        print(\"%s \\t | %.4f \\t\" % (\n",
    "            model_name[:12], eval_dict[model_name]['cv']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
